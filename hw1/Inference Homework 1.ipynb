{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Homework 1\n",
    "## Alex Pine, 2015/09/12\n",
    "### TODO\n",
    "I collaborated with Peter Li on this homework. TODO email address!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "A rare disease is good news, because even a test that is 99% accurate still has a high false positive rate. The chance that you have the disease given a positive test is only 0.4%, as shown by the following logic:\n",
    "\n",
    "Given probabilities:\n",
    "$$p(T = \\text{pos} | D = \\text{true} = 0.99, p(D = \\text{true}) = \\frac{1}{25000}$$\n",
    "\n",
    "Marginalize $p(T = \\text{pos})$:\n",
    "$$ p(T = \\text{pos}) = p(T = \\text{pos} | D = \\text{true})p(D = \\text{true}) + p(T = \\text{pos} | D = \\text{false})p(D = \\text{false})$$\n",
    "$$p(T = \\text{pos}) = 0.99 \\frac{1}{25000} + .01 \\frac{24999}{25000}$$\n",
    "\n",
    "Put it together using Bayes' rule:\n",
    "$$p(D = \\text{true} | T = \\text{pos}) = \\frac{p(T = \\text{pos} | D = \\text{true})p(D = \\text{true})}{p(T = \\text{pos})}$$\n",
    "\n",
    "$$p(D = \\text{true} | T = \\text{pos}) = \\frac{0.99 \\frac{1}{25000}}{0.99 \\frac{1}{25000} + .01 \\frac{24999}{25000}}$$\n",
    "\n",
    "$$p(D = \\text{true} | T = \\text{pos}) = \\frac{99}{99 + 24999} = 0.00394 ~= 0.4\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### 2.a $p(X_2 = \\text{Happy})$ \n",
    "\n",
    "$$ p(X_2 = \\text{Happy}) = p(X_2 = \\text{Happy} | X_1 = \\text{Happy})p(X_1 = \\text{Happy}) + p(X_2 = \\text{Happy} | X_1 = \\text{Angry})p(X_1 = \\text{Angry}) $$\n",
    "\n",
    "$$ p(X_2 = \\text{Happy}) = 0.9(1.0) + 0.1(0.0)$$\n",
    "\n",
    "$$ p(X_2 = \\text{Happy}) = 0.9 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b $p(Y_2 = \\text{Frown})$ \n",
    "\n",
    "$$ p(Y_2 = \\text{Frown}) = p(Y_2 = \\text{Frown} | X_2 = \\text{Happy})p(X_2 = \\text{Happy}) + p(Y_2 = \\text{Frown} | X_2 = \\text{Angry})p(X_2 = \\text{Angry}) $$\n",
    "\n",
    "$$ p(Y_2 = \\text{Frown}) = 0.1(0.9) + 0.6(1.0 - 0.9)$$\n",
    "\n",
    "$$ p(Y_2 = \\text{Frown}) = 0.63 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c $ p(X_2 = \\text{Happy} | Y_2 = \\text{Frown}) $\n",
    "\n",
    "$$ p(X_2 = \\text{Happy} | Y_2 = \\text{Frown}) =\n",
    "     \\frac{p(Y_2 = \\text{Frown} | X_2 = \\text{Happy}) p(X_2 = \\text{Happy})}\n",
    "     {p(Y_2 = \\text{Frown})} $$\n",
    "     \n",
    "$$ p(X_2 = \\text{Happy} | Y_2 = \\text{Frown}) = \\frac{0.1(0.9)}{0.63} $$\n",
    "\n",
    "$$ p(X_2 = \\text{Happy} | Y_2 = \\text{Frown}) = 0.143 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d $ p(Y_{80} = \\text{yell}) $\n",
    "\n",
    "$ p(Y_{80} = \\text{Yell})$ is given by the following formula:\n",
    "\n",
    "$$ p(Y_{80} = \\text{Yell}) = p(Y_{80} = \\text{Yell} | X_{80} = \\text{Happy})p(X_{80} = \\text{Happy}) + p(Y_{80} = \\text{Yell} | X_{80} = \\text{Angry})p(X_{80} = \\text{Angry}) $$\n",
    "\n",
    "To compute this, we need to find an expression for $p(X_{80} = \\text{Happy})$. The reccurance relation for $p(X_t = \\text{Happy})$ is given by the following:\n",
    "\n",
    "$$ p(X_t = H) = 0.9 p(X_{t-1} = H) + 0.1(1 - p(X_{t-1} = H)) $$\n",
    "\n",
    "$$ p(X_t = H) = 0.8p(X_{t-1} = H) + 0.1 $$\n",
    "\n",
    "This recurrance relation has the following closed form:\n",
    "$$  p(X_t = H) = (0.8)^{t-1} + 0.1\\sum_{i=1}^{t-1} (0.8)^{i-1} $$\n",
    "\n",
    "Remembering the following rule for geometric series:\n",
    "$$ \\sum_{k=1}^{n} a r^{k-1} = \\frac{a(1-r^n)}{1-r} $$\n",
    "Our expression for $p(X_t)$ becomes:\n",
    "\n",
    "$$  p(X_t = H) = (0.8)^{t-1} + \\frac{0.1(1-0.8^{t-1})}{1-0.8} $$\n",
    "\n",
    "$$ p(X_t = H) = \\frac{1}{2}(1 + 0.8^{t-1}) $$\n",
    "\n",
    "When $t = 80$, this becomes\n",
    "\n",
    "$$ p(X_{80} = H) = \\frac{1}{2}(1 + 0.8^{79}) $$\n",
    "\n",
    "Reusing the expression for $ p(Y_{80} = \\text{Yell})$ given above and the transition probabilities given in the problem description, we get:\n",
    "\n",
    "$$ p(Y_{80} = \\text{Yell}) = 0.1p(X_{80} = H) + 0.2(1 - p(X_{80} = H))$$ \n",
    "\n",
    "$$ p(Y_{80} = \\text{Yell}) = 0.1 (\\frac{1}{2}(1 + 0.8^{79})) + 0.2 (\\frac{1}{2}(1 - 0.8^{79}))$$ \n",
    "\n",
    "$$ p(Y_{80} = \\text{Yell}) = \\frac{1}{20}(3 - 0.8^{79}) $$\n",
    "\n",
    "$$ p(Y_{80} = \\text{Yell}) = 0.14999999889 $$\n",
    "\n",
    "As $t$ goes to infinity, this expression converges to $\\frac{3}{20}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.e $ \\text{argmax}_{x_1,\\ldots,x_5} \\big[ p(X_1 = x_1,\\ldots, X_5 = x_5 | Y_1 = Y_2 = Y_3 = Y_4 = Y_5 = \\text{frown}) \\big]$\n",
    "\n",
    "Bayes' rule states that:\n",
    "\n",
    "$$ p(X_1,X_2, X_3, X_4, X_5 | Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown}) = \\frac{p(X_1,X_2, X_3, X_4, X_5, Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown})}{p(Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown})} $$\n",
    "\n",
    "\n",
    "Since $p(Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown})$ is a constant, this expression is maximized when $p(X_1,X_2, X_3, X_4, X_5, Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown})$ is maximized.\n",
    "\n",
    "Since this distribution is an HMM, the joint distribution can be expressed as:\n",
    "\n",
    "$$ p(X_1, X_2, X_3, X_4, X_5, Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown}) = p(X_1 = H)p(Y_1 = F | X_1 = H)\\prod_{i=2}^{5} p(X_i = H | X_{i-1 = H})p(Y_i = F| X_i = H) $$\n",
    "\n",
    "Since We know $X_1 = H$, we need to find the sequence of $X_i, i = 2,\\ldots,5$ that maximizes this expression. The following code tries all sequences of \"happy\" and \"angry\", and picks the one with the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('H', 'H', 'H', 'H') . Joint Likelihood: 6.561e-05\n",
      "('H', 'H', 'H', 'A') . Joint Likelihood: 4.374e-05\n",
      "('H', 'H', 'A', 'H') . Joint Likelihood: 4.86e-06\n",
      "('H', 'H', 'A', 'A') . Joint Likelihood: 0.00026244\n",
      "('H', 'A', 'H', 'H') . Joint Likelihood: 4.86e-06\n",
      "('H', 'A', 'H', 'A') . Joint Likelihood: 3.24e-06\n",
      "('H', 'A', 'A', 'H') . Joint Likelihood: 2.916e-05\n",
      "('H', 'A', 'A', 'A') . Joint Likelihood: 0.00157464\n",
      "('A', 'H', 'H', 'H') . Joint Likelihood: 4.86e-06\n",
      "('A', 'H', 'H', 'A') . Joint Likelihood: 3.24e-06\n",
      "('A', 'H', 'A', 'H') . Joint Likelihood: 3.6e-07\n",
      "('A', 'H', 'A', 'A') . Joint Likelihood: 1.944e-05\n",
      "('A', 'A', 'H', 'H') . Joint Likelihood: 2.916e-05\n",
      "('A', 'A', 'H', 'A') . Joint Likelihood: 1.944e-05\n",
      "('A', 'A', 'A', 'H') . Joint Likelihood: 0.00017496\n",
      "('A', 'A', 'A', 'A') . Joint Likelihood: 0.00944784\n",
      "X_,...,X_5 MAP sequence: ('A', 'A', 'A', 'A') . Joint Likelihood: 0.00944784\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "best_sequence = None\n",
    "best_likelihood = 0.0\n",
    "for sequence in itertools.product(['H', 'A'], repeat=4):\n",
    "    states = [state for state in sequence]\n",
    "    likelihood = 1.0\n",
    "    for i, state in enumerate(states):\n",
    "        x_prob = 0.0\n",
    "        if i == 0:\n",
    "            x_prob = 0.9 if state == 'H' else 0.1\n",
    "        else:\n",
    "            x_prob = 0.9 if states[i] == states[i-1] else 0.1\n",
    "        y_prob = 0.1 if state == 'H' else 0.6\n",
    "        likelihood *= x_prob * y_prob\n",
    "    print sequence, '. Joint Likelihood:', likelihood\n",
    "    if not best_sequence or likelihood > best_likelihood:\n",
    "        best_sequence = sequence\n",
    "        best_likelihood = likelihood\n",
    "print 'X_,...,X_5 MAP sequence:', best_sequence, '. Joint Likelihood:', best_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of states that maximizes $ p(X_1,X_2, X_3, X_4, X_5 | Y1 = Y2 = Y3 = Y4 = Y5 = \\text{frown}) $ is\n",
    "\n",
    "$ X_1 = \\text{happy}, X_2 = \\text{angry}, X_3 = \\text{angry}, X_4 = \\text{angry}, X_5 = \\text{angry} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Let G a graph with two nodes: $X_1$ and $X_2$, where both nodes have a directed edge from one to the other. This graphs encodes the following joint probability distribution: $ p(X_1, X_2) = p(X_1|X_2)p(X_2|X_1) $.\n",
    "\n",
    "To show that this factorization is not a valid probability distribution, I'll pick arbitrary values for $p(X_1|X_2)$ and $p(X_2|X_1) $.\n",
    "\n",
    "This table encodes the CPD of $p(X_1|X_2)$, with the values of $X_1$ in the columns the values of $X_2$ in the rows.\n",
    "\n",
    "|   | 0  | 1  |\n",
    "|---|----|----|\n",
    "| 0 | .3 | .7 |\n",
    "| 1 | .1 | .9 |\n",
    "\n",
    "This table encodes the CPD of $p(X_2|X_1)$, with the values of $X_2$ in the columns the values of $X_1$ in the rows.\n",
    "\n",
    "|   | 0  | 1  |\n",
    "|---|----|----|\n",
    "| 0 | .2 | .8 |\n",
    "| 1 | .5 | .5 |\n",
    "\n",
    "Then we can calculate all the different values of $ p(X_1, X_2) $, since $ p(X_1, X_2) = p(X_1|X_2)p(X_2|X_1) $:\n",
    "\n",
    "$$ p(X_1 = 1, X_2 = 1) = 0.9(0.5) = 0.45 $$\n",
    "$$ p(X_1 = 1, X_2 = 0) = 0.7(0.5) = 0.35 $$\n",
    "$$ p(X_1 = 0, X_2 = 1) = 0.1(0.8) = 0.08 $$\n",
    "$$ p(X_1 = 0, X_2 = 0) = 0.3(0.2) = 0.06 $$\n",
    "\n",
    "Adding these probabilities up, $0.45 + 0.35 + 0.08 + 0.06 = 0.94$. This sum should equal 1, proving this is not a valid probability distribution, proving that graph does not encode a probability distribution, proving that cyclic graphs cannot encode probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Problem 4\n",
    "### 4.a\n",
    "$$ X_1 \\perp X_n, n = \\{2, 3, 5, 7, 8, 9, 10\\} $$\n",
    "$$ X_2 \\perp X_n, n = \\{1, 7, 8\\} $$\n",
    "$$ X_3 \\perp X_n, n = \\{1, 7, 8\\} $$\n",
    "$$ X_4 \\perp X_8 $$\n",
    "$$ X_5 \\perp X_1 $$\n",
    "$$ X_6 \\perp X_n, n = \\{7, 8\\} $$\n",
    "$$ X_7 \\perp X_n, n = \\{1, 2, 3, 6, 8, 10\\} $$\n",
    "$$ X_8 \\perp X_n, n = \\{1, 2, 3, 4, 6, 7, 10\\} $$\n",
    "$$ X_9 \\perp X_1 $$\n",
    "$$ X_{10} \\perp X_n, n = \\{1, 8, 9\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b\n",
    "\n",
    "$$ (X_1 \\perp X_A | \\{X_2, X_9\\}), A = \\{ 2, 3, 5, 7, 8, 9, 10 \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c\n",
    "\n",
    "$$ (X_8 \\perp X_B | \\{X_2, X_9\\}), B = \\{ 1, 5, 6 \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Show that there is no directed acyclic graph $G$ such that $I_{d-sep(G)} = I(p)$.\n",
    "\n",
    "Here is a table showing the joint probability distribution for $p(X, Y, Z)$:\n",
    "\n",
    "| x | y | z | XOR | p(x,y,z) |\n",
    "|---|---|---|-----|----------|\n",
    "| 0 | 0 | 0 | 0   | 1/12     |\n",
    "| 1 | 0 | 0 | 1   | 1/6      |\n",
    "| 0 | 1 | 0 | 1   | 1/6      |\n",
    "| 0 | 0 | 1 | 1   | 1/6      |\n",
    "| 1 | 1 | 0 | 0   | 1/12     |\n",
    "| 1 | 0 | 1 | 0   | 1/12     |\n",
    "| 0 | 1 | 1 | 0   | 1/12     |\n",
    "| 1 | 1 | 1 | 1   | 1/6      |\n",
    "\n",
    "First I will prove that $X \\perp Y$ and $X \\perp Z$.\n",
    "\n",
    "$$ p(X = 0) = p(X = 0, Y = 0, Z = 0) + p(X = 0, Y = 1, Z = 0) + p(X = 0, Y = 0, Z = 1) + p(X = 0, Y = 1, Z = 1) $$\n",
    "$$ p(X = 0) = \\frac{1}{12} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{12} $$\n",
    "$$ p(X = 0) = \\frac{1}{2} $$\n",
    "\n",
    "$$ p(X = 1) = p(X = 1, Y = 0, Z = 0) + p(X = 1, Y = 1, Z = 0) + p(X = 1, Y = 0, Z = 1) + p(X = 1, Y = 1, Z = 1) $$\n",
    "$$ p(X = 1) = \\frac{1}{6} + \\frac{1}{12} + \\frac{1}{12} + \\frac{1}{6} $$\n",
    "$$ p(X = 1) = \\frac{1}{2} $$\n",
    "\n",
    "From these calculations of $P(X)$, and the table above, it's clear that the calculations of $ P(X) = P(Y) = P(Z) = \\frac{1}{2}$. \n",
    "\n",
    "TODO Show that X and Y are independent, Y and Z are independent, and that X and Z are independent.\n",
    "\n",
    "TODO State that an unconnected graph encodes these independences. The graph is minimal b.c. you can't take away any more edges, because there are no edges.\n",
    "\n",
    "TODO That graph also implies P(X, Y, Z) = P(X)P(Y)P(Z) = 1/8, which isn't true, as shown by the CPT. Therefore there is no Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO problem 6\n",
    "\n",
    "immorality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "### a) Conditional Probability Tables\n",
    "| Forecast    |      |\n",
    "|-------------|------|\n",
    "| WillRain    | 0.30 |\n",
    "| WillNotRain | 0.70 |\n",
    "\n",
    "| Umbrella |          |             |\n",
    "|----------|----------|-------------|\n",
    "| Forecast | WillRain | WillNotRain |\n",
    "| Yes      | 0.75     | 0.05        |\n",
    "| No       | 0.25     | 0.95        |\n",
    "\n",
    "| Rain     |          |             |\n",
    "|----------|----------|-------------|\n",
    "| Forecast | WillRain | WillNotRain |\n",
    "| Yes      | 0.85     | 0.05        |\n",
    "| No       | 0.15     | 0.95        |\n",
    "\n",
    "| Drenched |      |     |     |     |\n",
    "|----------|------|-----|-----|-----|\n",
    "| Umbrella | Yes  | Yes | No  | No  |\n",
    "| Rain     | Yes  | No  | Yes | No  |\n",
    "| Yes      | 0.05 | 0.0 | 0.8 | 0.0 |\n",
    "| No       | 0.95 | 1.0 | 0.2 | 1.0 |\n",
    "\n",
    "| Cold     |      |     |\n",
    "|----------|------|-----|\n",
    "| Drenched | Yes  | No  |\n",
    "| Yes      | 0.90 | 0.5 |\n",
    "| No       | 0.10 | 0.5 |\n",
    "\n",
    "| Sprinkler |      |\n",
    "|-----------|------|\n",
    "| On        | 0.25 |\n",
    "| Off       | 0.75 |\n",
    "\n",
    "| Wet Grass |     |      |     |      |\n",
    "|-----------|-----|------|-----|------|\n",
    "| Rain      | Yes | Yes  | No  | No   |\n",
    "| Sprinkler | On  | Off  | On  | Off  |\n",
    "| Yes       | 1.0 | 0.95 | 1.0 | 0.01 |\n",
    "| No        | 0.0 | 0.05 | 0.0 | 0.99 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Perturb Forecast, observe effects\n",
    "#### Before perturbation\n",
    "Forecast CPD:\n",
    "\n",
    "| Forecast    |      |\n",
    "|-------------|------|\n",
    "| WillRain    | 0.30 |\n",
    "| WillNotRain | 0.70 |\n",
    "\n",
    "Marginal probability for Cold:\n",
    "\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5349 |\n",
    "| No       | 0.4651 |\n",
    "\n",
    "#### After perturbation:\n",
    "\n",
    "| Forecast    |      |\n",
    "|-------------|------|\n",
    "| WillRain    | 1.00 |\n",
    "| WillNotRain | 0.00 |\n",
    "\n",
    "Marginal probability for Cold:\n",
    "\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5807 |\n",
    "| No       | 0.4193 |\n",
    "\n",
    "#### Analysis\n",
    "Even though the prior probability for the WillRain forecast went from 30% to 100%, the probability of the Yes value for Cold only increased from 53.49% to 58.07%. Intuitively, this is because the effect of Forecast on Cold is diluted by the all the variables between them. Even if the weatherman forecasts 100% chance of rain, in order for that to result in me being cold, his prediction for rain must be accurate, I probably have to have forgetten my umbrella, and I probably had have gotten drenched. Even then, me being drenched is not a perfect predictor of me being cold. For the probability of Cold to increase significantly, the conditional probability distributions on the variables between it and Forecast must change as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) \"Cold = Yes\", noting its effect on \"Rain\"\n",
    "\n",
    "#### Before observing \"Cold = Yes\"\n",
    "\n",
    "| Rain     |          |\n",
    "|----------|----------|\n",
    "| Yes      | 0.29     |\n",
    "| No       | 0.71     |\n",
    "\n",
    "#### After observing \"Cold = Yes\"\n",
    "\n",
    "| Rain     |          |\n",
    "|----------|----------|\n",
    "| Yes      | 0.3363     |\n",
    "| No       | 0.6637     |\n",
    "\n",
    "#### Analysis\n",
    "After observing that \"Cold = Yes\", the marginal probability of it \"Rain = Yes\" increased from 29% to 33.63%. This is because Cold is influenced by Rain, so knowing that it is Cold makes it more likely that Rain was its underlying cause. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Effect of WetGrass on Cold\n",
    "#### Before observing WetGrass\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5349 |\n",
    "| No       | 0.4651 |\n",
    "\n",
    "#### After observing \"WetGrass = Yes\"\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5727 |\n",
    "| No       | 0.4273 |\n",
    "\n",
    "#### After observing \"WetGrass = No\"\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5024 |\n",
    "| No       | 0.4976 |\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "When the grass is wet, a likely reason for it was that it rained. Rain increases the likelihood of getting drenched, which increases the likelihood of getting cold. Similarly, if the grass is not wet, that decreases the likelihood that it rained, which decreases the likelihood of getting drenched, which decreases the likelihood of getting cold.\n",
    "To use the terminology from the \"Bayes Ball\" algorithm, since WetGrass is a child of Rain, observing its state allows  the ball to bounce from there to Rain. Since Drenched is a child of Rain, and Cold a child of Drenched, the ball can bounce all the way to Cold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Effect of Sprinkler and WetGrass on Cold\n",
    "#### Before observing either\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5349 |\n",
    "| No       | 0.4651 |\n",
    "\n",
    "#### After observing \"Sprinkler = Off\"\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5349 |\n",
    "| No       | 0.4651 |\n",
    "\n",
    "(No change)\n",
    "\n",
    "#### After observing \"Sprinkler = Off\" and \"WetGrass = Yes\"\n",
    "\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.6173 |\n",
    "| No       | 0.3827 |\n",
    "\n",
    "#### Analysis\n",
    "Observing the sprinkler's state alone has no effect on the marginal probability of it being cold. This is because Cold and Rain are both parents of WetGrass. The Bayes Ball algorithm states that information from one parent cannot flow through its child to another parent unless the child has also been observed. That is why, when WetGrass was set to \"Yes\", the likelihood of Cold increased. Now that we know the sprinkler was off, and that the grass is dry, it makes it all but certain that it rained, which increases the liklihood of being cold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Effect of a perfect forecast\n",
    "#### Marginal Probability of Cold with imperfect forecast\n",
    "| Rain     |          |             |\n",
    "|----------|----------|-------------|\n",
    "| Forecast | WillRain | WillNotRain |\n",
    "| Yes      | 0.85     | 0.05        |\n",
    "| No       | 0.15     | 0.95        |\n",
    "\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.5349 |\n",
    "| No       | 0.4651 |\n",
    "\n",
    "#### After perfect forecast\n",
    "\n",
    "| Cold     |      |\n",
    "|----------|------|\n",
    "| Yes      | 0.506 |\n",
    "| No       | 0.494 |\n",
    "\n",
    "#### Analysis\n",
    "If the forecast is perfectly predictive of rain, and I always bring my umbrella when it rains, then the liklihood of being cold decreases to 50.6% from 53.49%. This is because, in this situation, I always have my umbrella when it rains, making it less likely that I will get drenched, and thus less likely making it less likely that I get cold from being drenched. To test this, we can observe \"Rain = Yes\", and see that the marginal likelihood for Umbrella increases to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
