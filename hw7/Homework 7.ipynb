{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "## Alex Pine, 2015-12-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from pystruct.models import ChainCRF\n",
    "from pystruct.learners import OneSlackSSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "def read_input(data_dir, dataset_type):\n",
    "    assert dataset_type == 'train' or dataset_type == 'test', dataset_type\n",
    "    num_files = 5000 if dataset_type == 'train' else 1000\n",
    "    \n",
    "    X, y = [], []\n",
    "    # Iterate over all the training sample files\n",
    "    for f in [data_dir + \"/Data/\"+ dataset_type +\"-%d.txt\" % i \n",
    "              for i in range(1, num_files+1)]:\n",
    "        # Read each training sample file into 'data' variable\n",
    "        data = pd.read_csv(f, header=None, quoting=3)\n",
    "        # Extract 'tag' field into 'labels'\n",
    "        labels = data[1]\n",
    "        # Extract feature fields into 'features'\n",
    "        features = data.values[:, 2:].astype(np.int)\n",
    "        # Adjust features starting at 1 to start at 0\n",
    "        for f_idx in range(len(features)):\n",
    "          f1 = features[f_idx]\n",
    "          features[f_idx] = [f1[0]-1, f1[1], f1[2], f1[3]-1, f1[4]-1]\n",
    "        # Adjust labels to lie in {0,...,9}, and add to 'y'\n",
    "        y.append(labels.values - 1)\n",
    "        # Add feature vector to 'X'\n",
    "        X.append(features)\n",
    "\n",
    "    # See: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    # [Note: if you get an error on the below line, it may be because you need to\n",
    "    # upgrade scikit-learn]\n",
    "    encoder = OneHotEncoder(n_values=[1,2,2,201,201],sparse=False).fit(np.vstack(X))                 \n",
    "    # Represent features using one-of-K scheme: If a feature can take value in \n",
    "    # {0,...,K}, then introduce K binary features such that the value of only the \n",
    "    # i^th binary feature is non-zero when the feature takes value 'i'.\n",
    "    # n_values specifies the number of states each feature can take.\n",
    "    X_encoded = [encoder.transform(x) for x in X]\n",
    "    return X, X_encoded, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_orig, X_train_enc, y_train = read_input('ps7_data', 'train')\n",
    "X_test_orig, X_test_enc, y_test = read_input('ps7_data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# Problem 1: Find the value of the regularization hyperparameter \"C\" that\n",
    "# minimizes the loss function of the SSVM.\n",
    "# To do this, we maximize the 'score' value of the SSVM.\n",
    "\n",
    "# TODO Write a function that trains an SSVM and returns the weight vector, the \n",
    "#      test score, and the training score. Inputs should be the training and \n",
    "#      test sets.\n",
    "# \n",
    "# TODO Write function that finds an optimal value of C as measured by the\n",
    "#      SSVM's score function. Use grid search?\n",
    "#\n",
    "# TODO Find the optimal C for a model trained on the first 4500 training inputs.\n",
    "#      Then report the training and test error.\n",
    "#\n",
    "# TODO Train a model on all the training data using the value of C you found\n",
    "#      before, and report it's test error on the data in the test_XXX.txt files.\n",
    "\n",
    "def split_train_test(X, y, num_train):\n",
    "    TEST_SET_SIZE = 500\n",
    "    assert len(X) >= num_train + TEST_SET_SIZE, len(X)\n",
    "    assert len(y) >= num_train + TEST_SET_SIZE, len(y)\n",
    "    return X[:num_train], y[:num_train], X[:-TEST_SET_SIZE], y[:-TEST_SET_SIZE]\n",
    "\n",
    "\n",
    "# TODO what effect does the 'directed' param have? \n",
    "def learn_pos_weights(X_train, y_train, X_test, y_test, C):\n",
    "    # Construct a directed ChainCRF with 10 states for each variable, \n",
    "    # and pass this CRF to OneSlackSSVM constructor to create an object 'ssvm'\n",
    "    # Learn Structured SVM using X_small and y_small\n",
    "    crf = ChainCRF(n_states=10, inference_method='max-product', directed=True)\n",
    "    ssvm = OneSlackSSVM(crf, max_iter=200, C=C)\n",
    "    ssvm.fit(X_train, y_train)\n",
    "    # Store learnt weights in 'weights'\n",
    "    w = ssvm.w                  \n",
    "    # Evaluate training accuracy on X_small, y_small\n",
    "    train_score = ssvm.score(X_train, y_train)\n",
    "    # Get predicted labels on X_small using the learnt model\n",
    "    # print ssvm.predict(X_small)\n",
    "    test_score = ssvm.score(X_test, y_test)\n",
    "    return w, train_score, test_score\n",
    "\n",
    "# TODO consider using grid search from sklearn since it can be parallelized\n",
    "def find_best_model(X_train, y_train, X_test, y_test):\n",
    "    params = [0.1, 1.0, 10] # TODO no idea what good values are, TODO expand this\n",
    "    weights = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for C in params:\n",
    "        w, train_score, test_score = learn_pos_weights(X_train, y_train, \n",
    "                                                       X_test, y_test, C)\n",
    "        weights.append(w)\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    # TODO you should probably graph this\n",
    "    best_index = test_scores.index(max(test_scores))\n",
    "    return (params[best_index], weights[best_index], train_scores[best_index],\n",
    "            test_scores[best_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "#TODO 4500\n",
    "X, y, X_val, y_val = split_train_test(X_train_enc, y_train, 4500)\n",
    "\n",
    "C, w, train_score, test_score = find_best_model(X, y, X_val, y_val)\n",
    "\n",
    "print C, w, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
