{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "## Alex Pine, 2015-12-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from pystruct.models import ChainCRF\n",
    "from pystruct.learners import OneSlackSSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "def read_input(data_dir, dataset_type):\n",
    "    assert dataset_type == 'train' or dataset_type == 'test', dataset_type\n",
    "    num_files = 5000 if dataset_type == 'train' else 1000\n",
    "    \n",
    "    X, y = [], []\n",
    "    # Iterate over all the training sample files\n",
    "    for f in [data_dir + \"/Data/\"+ dataset_type +\"-%d.txt\" % i \n",
    "              for i in range(1, num_files+1)]:\n",
    "        # Read each training sample file into 'data' variable\n",
    "        data = pd.read_csv(f, header=None, quoting=3)\n",
    "        # Extract 'tag' field into 'labels'\n",
    "        labels = data[1]\n",
    "        # Extract feature fields into 'features'\n",
    "        features = data.values[:, 2:].astype(np.int)\n",
    "        # Adjust features starting at 1 to start at 0\n",
    "        for f_idx in range(len(features)):\n",
    "          f1 = features[f_idx]\n",
    "          features[f_idx] = [f1[0]-1, f1[1], f1[2], f1[3]-1, f1[4]-1]\n",
    "        # Adjust labels to lie in {0,...,9}, and add to 'y'\n",
    "        y.append(labels.values - 1)\n",
    "        # Add feature vector to 'X'\n",
    "        X.append(features)\n",
    "\n",
    "    # See: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    # [Note: if you get an error on the below line, it may be because you need to\n",
    "    # upgrade scikit-learn]\n",
    "    encoder = OneHotEncoder(n_values=[1,2,2,201,201],sparse=False).fit(np.vstack(X))                 \n",
    "    # Represent features using one-of-K scheme: If a feature can take value in \n",
    "    # {0,...,K}, then introduce K binary features such that the value of only the \n",
    "    # i^th binary feature is non-zero when the feature takes value 'i'.\n",
    "    # n_values specifies the number of states each feature can take.\n",
    "    X_encoded = [encoder.transform(x) for x in X]\n",
    "    return X, X_encoded, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_orig, X_train_enc, y_train = read_input('ps7_data', 'train')\n",
    "X_test_orig, X_test_enc, y_test = read_input('ps7_data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO delete\n",
    "# TODO what effect does the 'directed' param have? \n",
    "def learn_pos_weights(X_train, y_train, X_test, y_test, C):\n",
    "    # Construct a directed ChainCRF with 10 states for each variable, \n",
    "    # and pass this CRF to OneSlackSSVM constructor to create an object 'ssvm'\n",
    "    # Learn Structured SVM using X_small and y_small\n",
    "    crf = ChainCRF(n_states=10, inference_method='max-product', directed=True)\n",
    "    ssvm = OneSlackSSVM(crf, max_iter=200, C=C)\n",
    "    ssvm.fit(X_train, y_train)\n",
    "    # Store learnt weights in 'weights'\n",
    "    w = ssvm.w                  \n",
    "    # Evaluate training accuracy on X_small, y_small\n",
    "    train_score = ssvm.score(X_train, y_train)\n",
    "    # Get predicted labels on X_small using the learnt model\n",
    "    # print ssvm.predict(X_small)\n",
    "    test_score = ssvm.score(X_test, y_test)\n",
    "    return w, train_score, test_score\n",
    "\n",
    "# TODO consider using grid search from sklearn since it can be parallelized\n",
    "def find_best_reg_param(X_train, y_train, X_test, y_test):\n",
    "    params = [0.1, 1.0, 5.0] # TODO no idea what good values are, TODO expand this\n",
    "    weights = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for C in params:\n",
    "        w, train_score, test_score = learn_pos_weights(X_train, y_train, \n",
    "                                                       X_test, y_test, C)\n",
    "        weights.append(w)\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    # TODO you should probably graph this\n",
    "    best_index = test_scores.index(max(test_scores))\n",
    "    return (params[best_index], weights[best_index], train_scores[best_index],\n",
    "            test_scores[best_index])\n",
    "\n",
    "\n",
    "def find_best_model_OLD(X_train_enc, y_train, X_test_enc, y_test, num_train):\n",
    "    X, y, X_val, y_val = split_train_test(X_train_enc, y_train, num_train)\n",
    "\n",
    "    C, w, train_score, test_score = find_best_reg_param(X, y, X_val, y_val)\n",
    "\n",
    "    full_w, full_train_score, full_test_score = learn_pos_weights(\n",
    "        X_train_enc, y_train, X_test_enc, y_test, C)    \n",
    "    return C, w, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# Problem 1: Find the value of the regularization hyperparameter \"C\" that\n",
    "# minimizes the loss function of the SSVM.\n",
    "# To do this, we maximize the 'score' value of the SSVM.\n",
    "\n",
    "# TODO Write a function that trains an SSVM and returns the weight vector, the \n",
    "#      test score, and the training score. Inputs should be the training and \n",
    "#      test sets.\n",
    "# \n",
    "# TODO Write function that finds an optimal value of C as measured by the\n",
    "#      SSVM's score function. Use grid search?\n",
    "#\n",
    "# TODO Find the optimal C for a model trained on the first 4500 training inputs.\n",
    "#      Then report the training and test error.\n",
    "#\n",
    "# TODO Train a model on all the training data using the value of C you found\n",
    "#      before, and report it's test error on the data in the test_XXX.txt files.\n",
    "#\n",
    "# TODO Make a graph of train and test error for different reg params\n",
    "\n",
    "def split_train_test(X, y, num_train):\n",
    "    TEST_SET_SIZE = 500\n",
    "    assert len(X) >= num_train + TEST_SET_SIZE, len(X)\n",
    "    assert len(y) >= num_train + TEST_SET_SIZE, len(y)\n",
    "    return X[:num_train], y[:num_train], X[-TEST_SET_SIZE:], y[-TEST_SET_SIZE:]\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, Cs):\n",
    "    assert len(X_train) == len(y_train), 'x: %s y: %s' % (len(X_train), len(y_train))\n",
    "    assert len(X_val) == len(y_val), 'x: %s y: %s' % (len(X_val), len(y_val))\n",
    "\n",
    "    crf = ChainCRF(n_states=10, inference_method='max-product', directed=True)\n",
    "    best_model = None\n",
    "    best_C = None\n",
    "    smallest_error = None\n",
    "    print 'training model on data of size', len(X_train)\n",
    "    for C in Cs:\n",
    "        print 'training model with C =', C\n",
    "        ssvm = OneSlackSSVM(crf, max_iter=200, C=C)\n",
    "        ssvm.fit(X_train, y_train)\n",
    "        error = 1 - ssvm.score(X_val, y_val) # TODO 1- needed?\n",
    "        if not smallest_error or error < smallest_error:\n",
    "            best_model = ssvm\n",
    "            best_C = C\n",
    "            smallest_error = error\n",
    "    return best_model, best_C, smallest_error\n",
    "\n",
    "\n",
    "def train_models(X, y, num_train_points_list):\n",
    "    model_tuples = []\n",
    "    Cs = [0.1, 1.0, 10.0] # TODOTODOTODOTODOTODOTODO\n",
    "    for num_train in num_train_points_list:\n",
    "        X_train, y_train, X_val, y_val = split_train_test(X, y, num_train)\n",
    "        model, C, train_error = train_model(X_train, y_train, X_val, y_val, Cs)\n",
    "        model_tuples.append((model, C, train_error))\n",
    "    return model_tuples\n",
    "    \n",
    " \n",
    "def problem_one_find_best_C(X_train, y_train, X_test, y_test):\n",
    "    model, C, train_error = train_models(X_train, y_train, [4500])[0]\n",
    "    test_error = 1 - model.score(X_test, y_test)\n",
    "    return model, C, train_error, test_error\n",
    "\n",
    "\n",
    "def problem_one_full_model(X_train, y_train, X_test, y_test, best_C):\n",
    "    model, C, error = train_model(X_train, y_train, X_test, y_test, [best_C])\n",
    "    return model, error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training first Problem 1 model:\n",
      "training model on data of size 4500\n",
      "training model with C = 0.1\n",
      "training model with C = 1.0\n",
      "training model with C = 10.0\n"
     ]
    }
   ],
   "source": [
    "print 'Training first Problem 1 model:'\n",
    "\n",
    "prob_one_model, best_C, train_error, test_error = problem_one_find_best_C(\n",
    "    X_train_enc, y_train, X_test_enc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C 0.1 , training error: 0.116533949824 , validation error: 0.120678322598\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "\n",
    "print 'Best C', best_C, ', training error:', train_error, ', validation error:', test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_one_full_model, test_error = problem_one_full_model(\n",
    "    X_train, y_train, X_test, y_test, best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# Question 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_problem_two_models(X_train, y_train, X_test, y_test, \n",
    "                             prob_one_model_tuple):\n",
    "    num_train_points_list = [100, 200, 500, 1000]\n",
    "    model_tuples = train_models(X_train, y_train, num_train_points_list)\n",
    "    \n",
    "    num_train_points_list.append(4500) # TODO arg?\n",
    "    model_error_pairs.append(prob_one_tuple)\n",
    "    \n",
    "    models = [tup[0] for tup in model_tuples]\n",
    "    Cs = [tup[1] for tup in model_tuples]\n",
    "    train_errors = [tup[2] for tup in model_tuples]\n",
    "    test_errors = [1 - model.score(X_test, y_test) for model in models]\n",
    "    return zip(models, Cs, num_train_points_list, train_errors, test_errors)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_two_model_tuple = train_problem_two_models(\n",
    "    X_train, y_train, X_test, y_test, prob_one_model_tuple)\n",
    "models, Cs, num_train_points_list, train_errors, test_errors = *problem_two_model_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_errors(num_train_points_list, train_errors, test_errors):\n",
    "    fig = plt.figure(1, figsize=(10, 8))\n",
    "    fig.subtitle('Number of training points vs Errors', fontsize=20)\n",
    "    plt.xlabel('Number of training points', fontsize=15)\n",
    "    plt.ylabel('Error', fontsize=15) \n",
    "    train_line, = plt.plot(num_train_points_list, train_errors)    \n",
    "    test_line, = plt.plot(num_train_points_list, test_errors)\n",
    "    plt.legend([train_line, test_line], \n",
    "               ['Train Error', 'Test Error],\n",
    "               loc='upper right')\n",
    "    #TODOplt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "plot_errors(num_train_points_list, train_errors, test_errors)                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
